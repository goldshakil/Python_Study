{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework6-2017314461-Muhammad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLbyeea9NaL",
        "colab_type": "text"
      },
      "source": [
        "## 2017314461 Muhammad Shakeel Zuhaib\n",
        "Homework 6 (AI Porject)\n",
        "\n",
        "CIFAR 10 with CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS63jJoE9NJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Headers Definition\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k0Nz8N69U5y",
        "colab_type": "text"
      },
      "source": [
        "###Loading and splitting the data (Train/ Test/ Validate):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2lVktn8-MIF",
        "colab_type": "code",
        "outputId": "43f0df57-a340-4ed8-fea8-5d2841d6288c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Transformations\n",
        "transform = transforms.Compose([      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "                                     ]) #normalize each channel =>image = (image - mean) / std\n",
        "\n",
        "transform_test = transforms.Compose([ transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "                                     ]) #normalize each channel =>image = (image - mean) / std\n",
        "\n",
        "                  \n",
        "\n",
        "\n",
        "#loading the data and preprocessing it\n",
        "CIFAR_train= torchvision.datasets.CIFAR10(\"./data\",train=True, download=True, transform=transform) #Training Data\n",
        "CIFAR_test= torchvision.datasets.CIFAR10(\"./data\",train=False, download=True, transform=transform_test) #Testing Data\n",
        "\n",
        "#Create Validation Set\n",
        "indices = list(range(len(CIFAR_train)))\n",
        "seed=30\n",
        "np.random.seed(seed)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "split = int(np.floor(0.9 * len(CIFAR_train)))\n",
        "tr_idx, val_idx = indices[:split], indices[split:]\n",
        "\n",
        "tr_sampler = SubsetRandomSampler(tr_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        " \n",
        "\n",
        "#How are we gonna iterate over the data?\n",
        "train_loader= torch.utils.data.DataLoader(CIFAR_train,batch_size=128,sampler=tr_sampler,num_workers=2) #batch_size : process the data in batches and make a better generalization\n",
        "valid_loader= torch.utils.data.DataLoader(CIFAR_train,batch_size=128,sampler=val_sampler,num_workers=2)\n",
        "test_loader= torch.utils.data.DataLoader(CIFAR_test,batch_size=128,shuffle=False,num_workers=2)  #shuffling the data makes a better generalization\n",
        "\n",
        "# data labels\n",
        "labels = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#each data has 4 images"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u86fEyIr8zN0",
        "colab_type": "text"
      },
      "source": [
        "###This cell is only for checking the visuals for one picture (You can skip this): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O3x8FVd-T-Z",
        "colab_type": "code",
        "outputId": "d9997ffe-1518-4362-8428-ce8af5044655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#every type you run this it is shuffled\n",
        "for data in train_loader:\n",
        "  print(data[0].shape)  # batch_size, # channels, #height, #width\n",
        "  break\n",
        "\n",
        "#Every data is a list of: 1.#(batch_size)images   2.#(batch_size)labels\n",
        "\n",
        "# show images\n",
        "plt.imshow(np.transpose(data[0][0], (1, 2, 0))) #replace 0 with 1 axis and 1 with 2 and 2 with 0  -> output: height,width ,channel\n",
        "plt.show\n",
        "print(labels[data[1][0]])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 32, 32])\n",
            "dog\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibutTwu4-WJy",
        "colab_type": "text"
      },
      "source": [
        "###Neural Network Definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORlt0PVG-YHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  #initialize your network strucutre\n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "\n",
        "    #Conv2d Parameters: input channels,output channels, size_kernel\n",
        "    #BatchNorm Goals: Normalize the outputs, parameters:(#output_channels/ node)\n",
        "\n",
        "    self.conv1a = nn.Conv2d(3, 32, 3,padding=1)\n",
        "    self.bn1a = nn.BatchNorm2d(32)      \n",
        "    self.conv1b = nn.Conv2d(32, 64, 3,padding=1)\n",
        "    self.bn1b = nn.BatchNorm2d(64)   \n",
        "    self.pool1 = nn.MaxPool2d(2, 2)   \n",
        "\n",
        "\n",
        "\n",
        "    self.conv2a = nn.Conv2d(64, 128, 3,padding=1)\n",
        "    self.bn2a = nn.BatchNorm2d(128)\n",
        "    self.conv2b = nn.Conv2d(128, 128, 3,padding=1)\n",
        "    self.bn2b = nn.BatchNorm2d(128)\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "  \n",
        "\n",
        "    self.conv3a = nn.Conv2d(128, 256, 3,padding=1)\n",
        "    self.bn3a = nn.BatchNorm2d(256)\n",
        "    self.conv3b = nn.Conv2d(256, 256, 3,padding=1)\n",
        "    self.bn3b = nn.BatchNorm2d(256)\n",
        "    self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    self.fc1 = nn.Linear(4096, 512)\n",
        "    self.bn4 = nn.BatchNorm1d(512)    #Goal Normalize the outputs, parameters:(#output_nodes)\n",
        "\n",
        "    self.fc2 = nn.Linear(512, 128)\n",
        "    self.bn5 = nn.BatchNorm1d(128)\n",
        "\n",
        "    self.fc3 = nn.Linear(128, 10) \n",
        "\n",
        "  #define your network inner functions  \n",
        "  def forward(self,x):\n",
        "    x = self.pool1(F.relu(self.bn1b(self.conv1b(self.bn1a(self.conv1a(x))))))  # Convolution -> Normalize -> threshold/activation(optional) -> Convolution  -> Normalize ->threshold/activation -> pooling\n",
        "    x = self.pool2(F.relu(self.bn2b(self.conv2b(self.bn2a(self.conv2a(x))))))\n",
        "    x = self.pool3(F.relu(self.bn3b(self.conv3b(self.bn3a(self.conv3a(x))))))  \n",
        "    #flatten\n",
        "    x = x.view(-1, 4096)\n",
        "    x = F.relu(self.bn4(self.fc1(x))) # input,connection_weight -> Normalize -> threshold/activation\n",
        "    x = F.relu(self.bn5(self.fc2(x)))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "    #this function will return for each picture: 10 nodes (prediction value of each label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUDrKlka-aJz",
        "colab_type": "text"
      },
      "source": [
        "###Driver Code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ5rD8mt-cg6",
        "colab_type": "code",
        "outputId": "420d01bc-7312-430a-ba26-a10f5770fc7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Select GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(\"This model is running on\" , torch.cuda.get_device_name())\n",
        "\n",
        "#Model\n",
        "net=Net().to(device)\n",
        "\n",
        "#Get adjustable parameters(weights) and optimize them \n",
        "optimizer=optim.Adam(net.parameters(),lr=0.001,weight_decay=0.0001) #weight decay is multiplied to weight to prevent them from growing too large\n",
        "\n",
        "#Error Function\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "# Learning rate scheduler: adjusts learning rate as the epoch increases\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) #Decays the learning rate by multiplyin by gamma every step_size epochs\n",
        "\n",
        "#How many times we pass our full data (the same data)\n",
        "total_epoch=50 "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "This model is running on Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-dRL5eb-etj",
        "colab_type": "text"
      },
      "source": [
        "###Training and Validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTKzpdZ6-iVJ",
        "colab_type": "code",
        "outputId": "aab39010-f359-4e0b-c1ee-0d11b5963233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_valid_acc=0\n",
        "\n",
        "for cur_epoch in range(total_epoch):\n",
        "  train_correct=0\n",
        "  train_total=0\n",
        "  train_loss=0 #loss per epoch\n",
        "\n",
        "  valid_correct=0\n",
        "  valid_total=0\n",
        "  valid_loss=0 #loss per epoch\n",
        "  \n",
        "  net.train() #put the model in training mode\n",
        "  for data in train_loader:\n",
        "\n",
        "    #every data consits of (batch_size) images\n",
        "    X,y=data[0].to(device), data[1].to(device) #picture(X batch_size), label(X batch_size) -> #batch size comes first #note that the label here is a number which is index in labels list\n",
        "    \n",
        "    net.zero_grad()  \n",
        "    output = net(X)  \n",
        "    loss = criterion(output, y) #calculate the error/ loss for the that batch (data)\n",
        "\n",
        "    loss.backward()  #computes dloss/dw for every parameter w  (loss for every parameter)\n",
        "    optimizer.step() #update weights\n",
        "    train_loss+=loss.item()\n",
        "\n",
        "    #calculate how many right do you have in every training data until the end of all training datas\n",
        "    #output is Batch_size*10 tensor\n",
        "    for k, i in enumerate(output): # the output is batch_size* 10 tensor   # k is the index of the data # i the data itself\n",
        "        if torch.argmax(i) == y[k]: # in every row find the highest prediction index and compare it to y[k]\n",
        "                train_correct += 1\n",
        "        train_total += 1\n",
        "\n",
        "  exp_lr_scheduler.step() #learning rate adjustment\n",
        "  \n",
        "  net.eval() #put the model in evaluation mode\n",
        "  #validate for each epoch\n",
        "  with torch.no_grad(): # no gradient\n",
        "    for data in valid_loader:\n",
        "      X, y = data[0].to(device), data[1].to(device) # store the images in X and labels in y\n",
        "      output = net(X) \n",
        "      loss = criterion(output, y)\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "      for k, i in enumerate(output): # the output is batch_size* 10 ARRAY\n",
        "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
        "              valid_correct += 1\n",
        "          valid_total += 1\n",
        "  \n",
        "  #if the model is better than the previous best store it\n",
        "  if((valid_correct/valid_total)>best_valid_acc):\n",
        "    best_valid_acc= (valid_correct/valid_total)\n",
        "    torch.save(net.state_dict(), \"./save_best.pth\") #save early stopping point\n",
        "\n",
        "  print(' Epoch {}/{}: Training Accuracy {} |  Training Loss {} || Validation Accuracy {} |  Validation Loss {}'.format(cur_epoch+1, total_epoch, train_correct/train_total,train_loss/len(train_loader),valid_correct/valid_total,valid_loss/len(valid_loader))) #accuray for each epoch\n",
        "  print(' Best validation so far {}'.format(best_valid_acc))\n",
        "  print('-------------------------------------------------------------------------------------------------------------------------------')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch 1/50: Training Accuracy 0.93 |  Training Loss 0.20097630164078015 || Validation Accuracy 0.843 |  Validation Loss 0.501394622772932\n",
            " Best validation so far 0.843\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 2/50: Training Accuracy 0.9336888888888889 |  Training Loss 0.189052396051755 || Validation Accuracy 0.8326 |  Validation Loss 0.5541829496622086\n",
            " Best validation so far 0.843\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 3/50: Training Accuracy 0.9401333333333334 |  Training Loss 0.17612645063887944 || Validation Accuracy 0.841 |  Validation Loss 0.5217827565968036\n",
            " Best validation so far 0.843\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 4/50: Training Accuracy 0.9393111111111111 |  Training Loss 0.17308179625648668 || Validation Accuracy 0.8356 |  Validation Loss 0.5089869260787964\n",
            " Best validation so far 0.843\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 5/50: Training Accuracy 0.9425777777777777 |  Training Loss 0.1632916554173624 || Validation Accuracy 0.8368 |  Validation Loss 0.5383928433060646\n",
            " Best validation so far 0.843\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 6/50: Training Accuracy 0.9466888888888889 |  Training Loss 0.15193483829286628 || Validation Accuracy 0.8398 |  Validation Loss 0.5590784549713135\n",
            " Best validation so far 0.843\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 7/50: Training Accuracy 0.9748 |  Training Loss 0.0791285450104624 || Validation Accuracy 0.8686 |  Validation Loss 0.4008179694414139\n",
            " Best validation so far 0.8686\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 8/50: Training Accuracy 0.9885111111111111 |  Training Loss 0.045333882661494004 || Validation Accuracy 0.8722 |  Validation Loss 0.4179171986877918\n",
            " Best validation so far 0.8722\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 9/50: Training Accuracy 0.9925111111111111 |  Training Loss 0.03466454039285467 || Validation Accuracy 0.876 |  Validation Loss 0.41121776178479197\n",
            " Best validation so far 0.876\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 10/50: Training Accuracy 0.9955555555555555 |  Training Loss 0.02629451126135378 || Validation Accuracy 0.8752 |  Validation Loss 0.4149917259812355\n",
            " Best validation so far 0.876\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 11/50: Training Accuracy 0.9969555555555556 |  Training Loss 0.020462419088951057 || Validation Accuracy 0.8782 |  Validation Loss 0.4206592973321676\n",
            " Best validation so far 0.8782\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 12/50: Training Accuracy 0.9971111111111111 |  Training Loss 0.017486118252484383 || Validation Accuracy 0.8802 |  Validation Loss 0.4236023060977459\n",
            " Best validation so far 0.8802\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 13/50: Training Accuracy 0.9983333333333333 |  Training Loss 0.013511812213850631 || Validation Accuracy 0.8762 |  Validation Loss 0.4789100717753172\n",
            " Best validation so far 0.8802\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 14/50: Training Accuracy 0.9986444444444444 |  Training Loss 0.011235093181974 || Validation Accuracy 0.8804 |  Validation Loss 0.43435720913112164\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 15/50: Training Accuracy 0.9992666666666666 |  Training Loss 0.008819535887927155 || Validation Accuracy 0.875 |  Validation Loss 0.4458838555961847\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 16/50: Training Accuracy 0.9993111111111111 |  Training Loss 0.007718790802755393 || Validation Accuracy 0.8792 |  Validation Loss 0.4397724088281393\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 17/50: Training Accuracy 0.9995111111111111 |  Training Loss 0.006120754485230215 || Validation Accuracy 0.8754 |  Validation Loss 0.44252754673361777\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 18/50: Training Accuracy 0.9994888888888889 |  Training Loss 0.005983219619586386 || Validation Accuracy 0.8748 |  Validation Loss 0.5018978089094162\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 19/50: Training Accuracy 0.9996222222222222 |  Training Loss 0.0051950127209155735 || Validation Accuracy 0.8756 |  Validation Loss 0.4874388616532087\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 20/50: Training Accuracy 0.9997777777777778 |  Training Loss 0.004513956161893227 || Validation Accuracy 0.8734 |  Validation Loss 0.48392500951886175\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 21/50: Training Accuracy 0.9996 |  Training Loss 0.004706509489386173 || Validation Accuracy 0.872 |  Validation Loss 0.5031545840203762\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 22/50: Training Accuracy 0.9999111111111111 |  Training Loss 0.003745919694748326 || Validation Accuracy 0.8734 |  Validation Loss 0.4924452040344477\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 23/50: Training Accuracy 0.9995333333333334 |  Training Loss 0.004662000603275374 || Validation Accuracy 0.872 |  Validation Loss 0.5508701376616955\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 24/50: Training Accuracy 0.9992666666666666 |  Training Loss 0.0053016220230016516 || Validation Accuracy 0.8702 |  Validation Loss 0.5278325274586677\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 25/50: Training Accuracy 0.9995111111111111 |  Training Loss 0.004733528070077723 || Validation Accuracy 0.8702 |  Validation Loss 0.5548600941896439\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 26/50: Training Accuracy 0.9989777777777777 |  Training Loss 0.00599496336045294 || Validation Accuracy 0.8658 |  Validation Loss 0.5548322476446629\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 27/50: Training Accuracy 0.9992888888888889 |  Training Loss 0.004963963838566666 || Validation Accuracy 0.871 |  Validation Loss 0.5534221842885018\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 28/50: Training Accuracy 0.9998666666666667 |  Training Loss 0.0031545328939418223 || Validation Accuracy 0.8718 |  Validation Loss 0.5188082292675972\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 29/50: Training Accuracy 0.9999111111111111 |  Training Loss 0.002622075837610861 || Validation Accuracy 0.8748 |  Validation Loss 0.5329578876495361\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 30/50: Training Accuracy 0.9998888888888889 |  Training Loss 0.0024035341550850057 || Validation Accuracy 0.8734 |  Validation Loss 0.5489214479923248\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 31/50: Training Accuracy 0.9999333333333333 |  Training Loss 0.0021866406165248586 || Validation Accuracy 0.8736 |  Validation Loss 0.5132376037538051\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 32/50: Training Accuracy 0.9999555555555556 |  Training Loss 0.0019361566525979222 || Validation Accuracy 0.8754 |  Validation Loss 0.5190261013805866\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 33/50: Training Accuracy 0.9999777777777777 |  Training Loss 0.0017827089680147103 || Validation Accuracy 0.8766 |  Validation Loss 0.5083658210933208\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 34/50: Training Accuracy 1.0 |  Training Loss 0.0017187651412148791 || Validation Accuracy 0.8732 |  Validation Loss 0.5268609326332807\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 35/50: Training Accuracy 0.9999333333333333 |  Training Loss 0.0016538281581068243 || Validation Accuracy 0.874 |  Validation Loss 0.5014831881970168\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 36/50: Training Accuracy 0.9999777777777777 |  Training Loss 0.001516865130642228 || Validation Accuracy 0.8792 |  Validation Loss 0.50115772113204\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 37/50: Training Accuracy 1.0 |  Training Loss 0.0014144558061327023 || Validation Accuracy 0.8724 |  Validation Loss 0.522374564409256\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 38/50: Training Accuracy 1.0 |  Training Loss 0.001240049334724476 || Validation Accuracy 0.8762 |  Validation Loss 0.5078556023538112\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 39/50: Training Accuracy 1.0 |  Training Loss 0.00123745323435287 || Validation Accuracy 0.8758 |  Validation Loss 0.5022196657955647\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 40/50: Training Accuracy 0.9999777777777777 |  Training Loss 0.0013024934821508148 || Validation Accuracy 0.8758 |  Validation Loss 0.5110047601163388\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 41/50: Training Accuracy 1.0 |  Training Loss 0.0011794654452718203 || Validation Accuracy 0.8758 |  Validation Loss 0.515691889822483\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 42/50: Training Accuracy 1.0 |  Training Loss 0.0012120175107900816 || Validation Accuracy 0.877 |  Validation Loss 0.5239001609385013\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 43/50: Training Accuracy 1.0 |  Training Loss 0.001153695046510124 || Validation Accuracy 0.8782 |  Validation Loss 0.5002967990934849\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 44/50: Training Accuracy 0.9999777777777777 |  Training Loss 0.0011060579305525277 || Validation Accuracy 0.8772 |  Validation Loss 0.5149273693561554\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 45/50: Training Accuracy 1.0 |  Training Loss 0.001035569047830491 || Validation Accuracy 0.8782 |  Validation Loss 0.5082685589790344\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 46/50: Training Accuracy 1.0 |  Training Loss 0.0010233486179706895 || Validation Accuracy 0.8788 |  Validation Loss 0.506542750261724\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 47/50: Training Accuracy 1.0 |  Training Loss 0.0010164260369161968 || Validation Accuracy 0.8782 |  Validation Loss 0.5107953183352947\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 48/50: Training Accuracy 1.0 |  Training Loss 0.0009223825099153063 || Validation Accuracy 0.8756 |  Validation Loss 0.4970461577177048\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 49/50: Training Accuracy 1.0 |  Training Loss 0.0009830425611157393 || Validation Accuracy 0.8754 |  Validation Loss 0.5114681120961905\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            " Epoch 50/50: Training Accuracy 0.9999555555555556 |  Training Loss 0.0009676806036044251 || Validation Accuracy 0.8752 |  Validation Loss 0.5135895997285843\n",
            " Best validation so far 0.8804\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONxVfnsH-m_p",
        "colab_type": "text"
      },
      "source": [
        "###Testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5FXLoJN-oYr",
        "colab_type": "code",
        "outputId": "894f94f3-6615-4aa7-8800-423d9cd9d1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load the best validation accuracy model so far\n",
        "load_model = Net().to(device)\n",
        "load_model.load_state_dict(torch.load(\"./save_best.pth\")) \n",
        "\n",
        "load_model.eval()\n",
        "\n",
        "correct =0\n",
        "total=0\n",
        "with torch.no_grad(): # no gradient\n",
        "  for data in test_loader:\n",
        "      X, y = data[0].to(device), data[1].to(device) # store the images in X and labels in y\n",
        "      output = load_model(X) #send the 4 images\n",
        "      #print(output)\n",
        "      for k, i in enumerate(output): # the output is 4* 10 ARRAY\n",
        "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
        "              correct += 1\n",
        "          total += 1\n",
        "\n",
        "print(\"Test Accuracy: \", correct/total)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.8715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCGtZNQV2HT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}