{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YH_KaFBJOFTb"
   },
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVCvc3Y5Whtg"
   },
   "outputs": [],
   "source": [
    "#Headers Definition\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqFids7lPE9t"
   },
   "source": [
    "###Loading and splitting the data (Train/ Test/ Validate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SuZ8H3btWlsf",
    "outputId": "f0ec0911-ed9a-43a7-9fb5-3f4df48c17fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Transformations\n",
    "transform = transforms.Compose([      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ]) #normalize each channel =>image = (image - mean) / std\n",
    "\n",
    "\n",
    "#loading the data and preprocessing it\n",
    "CIFAR_train= torchvision.datasets.CIFAR10(\"./data\",train=True, download=True, transform=transform) #Training Data\n",
    "CIFAR_test= torchvision.datasets.CIFAR10(\"./data\",train=False, download=True, transform=transform) #Testing Data\n",
    "\n",
    "#Create Validation Set\n",
    "indices = list(range(len(CIFAR_train)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(0.9 * len(CIFAR_train)))\n",
    "tr_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "tr_sampler = SubsetRandomSampler(tr_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "# random_split() Issues: \n",
    "#tr, val = torch.utils.data.random_split(CIFAR_train, [int(len(CIFAR_train) * 0.9), int(len(CIFAR_train) * 0.1)])  \n",
    "\n",
    "#How are we gonna iterate over the data?\n",
    "train_loader= torch.utils.data.DataLoader(CIFAR_train,batch_size=4,sampler=tr_sampler) #batch_size : process the data in batches and make a better generalization\n",
    "valid_loader= torch.utils.data.DataLoader(CIFAR_train,batch_size=4,sampler=val_sampler)\n",
    "test_loader= torch.utils.data.DataLoader(CIFAR_test,batch_size=4,shuffle=False)  #shuffling the data makes a better generalization\n",
    "\n",
    "# data labels\n",
    "labels = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#each data has 4 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzgCuUGk0bIs"
   },
   "source": [
    "###This cell is only for checking the visuals for one picture (You can skip this): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "0bhP2QzdYnxU",
    "outputId": "684c600d-8fb4-4e32-e08c-d912762d5c71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "horse\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#every type you run this it is shuffled\n",
    "for data in train_loader:\n",
    "  print(data[0].shape)  # batch_size, # channels, #height, #width\n",
    "  break\n",
    "\n",
    "# show images\n",
    "plt.imshow(np.transpose(data[0][0], (1, 2, 0))) #replace 0 with 1 axis and 1 with 2 and 2 with 0  -> output: height,width ,channel\n",
    "plt.show\n",
    "print(labels[data[1][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hExW4SfwPsex"
   },
   "source": [
    "###Neural Network Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qzx74mFFOsr3"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__() #initialize your network\n",
    "    self.fc1=nn.Linear(32*32*3, 256) #32*32*3 -> first flattened image and fully connected layer\n",
    "    self.fc2=nn.Linear(256, 32) # second layer\n",
    "    self.fc3=nn.Linear(32, 10) #output layer has 10 neurons\n",
    "    \n",
    "  def forward(self,x):\n",
    "    x=F.relu(self.fc1(x)) # apply the activation function\n",
    "    x=F.relu(self.fc2(x)) # apply the activation function\n",
    "    x=self.fc3(x) # apply the activation function either zero or one ! so one of the node is fired \n",
    "    return F.log_softmax(x,dim=1)\n",
    "    #this function will return for each picture: 10 nodes (prediction value of each label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pt3IsmFaQRUS"
   },
   "source": [
    "###Driver Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyYJZ62-QTr8"
   },
   "outputs": [],
   "source": [
    "net=Net() #model\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001) #get adjustable parameters(weights) and optimize them\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "total_epoch=30 #How many times we pass our full data (the same data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIa5B6FdQc5D"
   },
   "source": [
    "###Training and Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ERxldKm4Qe7w",
    "outputId": "3e1fe4b7-483b-4e9d-f398-81ec6f0843e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/30: Training Accuracy 0.42848888888888886 |  Training Loss 1.6069304905288748 || Validation Accuracy 0.4198 |  Validation Loss 1.6288685871839523\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 6/30: Training Accuracy 0.45653333333333335 |  Training Loss 1.5336547691636615 || Validation Accuracy 0.4626 |  Validation Loss 1.52523183658123\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 9/30: Training Accuracy 0.4712 |  Training Loss 1.4944394145306614 || Validation Accuracy 0.4532 |  Validation Loss 1.5201287608981133\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 12/30: Training Accuracy 0.48104444444444444 |  Training Loss 1.4629944808873865 || Validation Accuracy 0.4654 |  Validation Loss 1.513323668050766\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 15/30: Training Accuracy 0.49135555555555555 |  Training Loss 1.4473572291738457 || Validation Accuracy 0.4728 |  Validation Loss 1.5023929282963275\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 18/30: Training Accuracy 0.49495555555555554 |  Training Loss 1.4248057428485816 || Validation Accuracy 0.4528 |  Validation Loss 1.5569343018054962\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 21/30: Training Accuracy 0.5010444444444444 |  Training Loss 1.418428341909912 || Validation Accuracy 0.496 |  Validation Loss 1.4519433022856711\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 24/30: Training Accuracy 0.5088222222222222 |  Training Loss 1.3967097292474575 || Validation Accuracy 0.4904 |  Validation Loss 1.4991957429170608\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 27/30: Training Accuracy 0.5130444444444444 |  Training Loss 1.3904310803405113 || Validation Accuracy 0.477 |  Validation Loss 1.5119353418827057\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 30/30: Training Accuracy 0.5162666666666667 |  Training Loss 1.3752381254888244 || Validation Accuracy 0.4932 |  Validation Loss 1.4769770957887172\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for cur_epoch in range(total_epoch):\n",
    "  train_correct=0\n",
    "  train_total=0\n",
    "  train_loss=0 #loss per epoch\n",
    "\n",
    "  valid_correct=0\n",
    "  valid_total=0\n",
    "  valid_loss=0 #loss per epoch\n",
    "  \n",
    "  for data in train_loader:\n",
    "    #every data consits of 4 (batch_size) images\n",
    "    X,y=data #picture(X batch_size), label(X batch_size) -> #batch size comes first #note that the label here is a number which is index in labels list\n",
    "    net.zero_grad()  \n",
    "    output = net(X.view(-1,32*32*3))  \n",
    "    loss = criterion(output, y) #calculate the error/ loss for the that batch (data)\n",
    "\n",
    "    loss.backward()  #computes dloss/dw for every parameter w  (loss for every parameter)\n",
    "    optimizer.step() #update weights\n",
    "    train_loss+=loss.item()\n",
    "\n",
    "    #calculate how many right do you have in every training data until the end of all training datas\n",
    "    #output is Batch_size*10 tensor\n",
    "    for k, i in enumerate(output): # the output is batch_size* 10 tensor   # k is the index of the data # i the data itself\n",
    "        if torch.argmax(i) == y[k]: # in every row find the highest prediction index and compare it to y[k]\n",
    "                train_correct += 1\n",
    "        train_total += 1\n",
    "\n",
    "  #validate for each epoch\n",
    "  with torch.no_grad(): # no gradient\n",
    "    for data in valid_loader:\n",
    "      X, y = data # store the images in X and labels in y\n",
    "      output = net(X.view(-1,32*32*3)) #send the 4 images\n",
    "      loss = criterion(output, y)\n",
    "\n",
    "      valid_loss += loss.item()\n",
    "\n",
    "      for k, i in enumerate(output): # the output is 4* 10 ARRAY\n",
    "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
    "              valid_correct += 1\n",
    "          valid_total += 1\n",
    "  \n",
    "  \n",
    "  if((cur_epoch+1)%(total_epoch*0.1)==0):\n",
    "    print(' Epoch {}/{}: Training Accuracy {} |  Training Loss {} || Validation Accuracy {} |  Validation Loss {}'.format(cur_epoch+1, total_epoch, train_correct/train_total,train_loss/len(train_loader),valid_correct/valid_total,valid_loss/len(valid_loader))) #accuray for each epoch\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rYhfRHKQmoG"
   },
   "source": [
    "###Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CsRP9l0_QoLX",
    "outputId": "547d9889-bebd-4904-eca4-ffa753d0b111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.478\n"
     ]
    }
   ],
   "source": [
    "correct =0\n",
    "total=0\n",
    "with torch.no_grad(): # no gradient\n",
    "  for data in test_loader:\n",
    "      X, y = data # store the images in X and labels in y\n",
    "      output = net(X.view(-1,32*32*3)) #send the 4 images\n",
    "      #print(output)\n",
    "      for k, i in enumerate(output): # the output is 4* 10 ARRAY\n",
    "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
    "              correct += 1\n",
    "          total += 1\n",
    "\n",
    "print(\"Test Accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzuSDnn0ol-V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework5-2017314461-Muhammad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
