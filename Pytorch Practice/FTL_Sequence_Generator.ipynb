{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FTL_Sequence_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtNgpw-qyb2B",
        "colab_type": "text"
      },
      "source": [
        
        "If you have any problem contact me at:\n",
        "\n",
        "1) goldshakil (kakao)\n",
        "\n",
        "2) omarshakil100@gmail.com (e-mail)\n",
        "\n",
        "###Seq2Seq modeling using LSTM\n",
        "###(1000 Sequence to 2000 Sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmraw5gZys6U",
        "colab_type": "text"
      },
      "source": [
        "### Importing Header Files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EvTHV_bxW4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing header files\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "import unicodedata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIwlEgodzIA8",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess the data here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qm-QncozMeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the code for preprocessing data here\n",
        "# the data should be represented as : (x,y) -> where x is input and y target\n",
        "# x: current address\n",
        "# y: next \n",
        "\n",
        "# load the data as follows:\n",
        "### train_loader: for training set\n",
        "### valid_loader: for validation set\n",
        "### test_loader: for training set"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrliqLgfzevo",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Model Class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3T-aU_rxe5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 8 # each batch  is 1000 Integers\n",
        "num_layers = 4 # fine tuning needed\n",
        "input_size = 1000 #The input sequence is 1000 Integers\n",
        "hidden_size = 128      #the output size of lstm (our choice)\n",
        "output_size=2000 #The output sequence is 2000 Integers\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Model, self).__init__() \n",
        "\n",
        "      # A look up table for embeddings \n",
        "      self.embedding=nn.Embedding(input_size,hidden_size)\n",
        "\n",
        "\n",
        "      # LSTM Layer: It outputs the hidden state (here the layer is more like a cell)\n",
        "      # Remember:\n",
        "        # input_size: is a single number represented by the hot vector size\n",
        "        # hidden_size: is our choice it is just an intermediate LSTM/RNN output size\n",
        "      # The input should be of shape (seq_len, batch_size, input_size)\n",
        "      # output1: outputs of shape (seq_len, batch, num_directions * hidden_size)\n",
        "      # output2: h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
        "      # output3: c_n of shape (num_layers * num_directions, batch, hidden_size)\n",
        "\n",
        "      self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        \n",
        "\n",
        "      # Fully connected layer: takes the hidden state and outputs a vector of size \"output_size\"\n",
        "      # here the output is categories\n",
        "      self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self,x,lengths):\n",
        "\n",
        "    # input -> embed -> pack -> LSTM -> unpack -> FC\n",
        "\n",
        "    batches=x.size(0) #input: Batches* Sequence\n",
        "\n",
        "    x=x.permute(1,0)  #input: Batches* Sequence -> output: Sequence* Batches\n",
        "\n",
        "    embedded=self.embedding(x) #input : sequence*batches -> output:  sequence*batches*embedding size\n",
        "\n",
        "    packed_input = pack_padded_sequence(embedded, lengths)\n",
        "\n",
        "    hidden1=self.initHidden(batches) #hidden shape: num_layers * num_directions, batch, hidden_size)\n",
        "    hidden2=self.initHidden(batches)\n",
        "\n",
        "    out, (h_n, c_n) = self.lstm(packed_input, (hidden1,hidden2)) # out shape: (seq_len, batch, num_directions * hidden_size)\n",
        "\n",
        "    #lstm_out, _ = pad_packed_sequence(packed_output)  -> no need for unpacking we can use the hidden state simpler\n",
        "    #print(out.data.shape)\n",
        "    h_n=h_n.permute(1,0,2)\n",
        "    out = self.fc(h_n.view(batches,-1)) #easier than using out\n",
        "    out=  nn.functional.log_softmax(out,dim=1)\n",
        "\n",
        "    return out\n",
        "\n",
        "  # a function for initializing the h_n, c_n\n",
        "  def initHidden(self,batches):\n",
        "    return torch.zeros(num_layers,batches, hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNtuMhUHzkqx",
        "colab_type": "text"
      },
      "source": [
        "### Driver Code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTUWPFntz0iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(\"This model is running on\" , torch.cuda.get_device_name())\n",
        "\n",
        "#Model\n",
        "net=Model().to(device)\n",
        "\n",
        "#Get adjustable parameters(weights) and optimize them \n",
        "optimizer=optim.Adam(net.parameters(),lr=0.001,weight_decay=0.0001) #weight decay is multiplied to weight to prevent them from growing too large\n",
        "\n",
        "#Error Function\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "# Learning rate scheduler: adjusts learning rate as the epoch increases\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) #Decays the learning rate by multiplyin by gamma every step_size epochs\n",
        "\n",
        "#How many times we pass our full data (the same data)\n",
        "total_epoch=50 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNLcPGYDz5c7",
        "colab_type": "text"
      },
      "source": [
        "### Training and Validation: \n",
        "Make sure your data is represented as described above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-V3w6HGzz_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for cur_epoch in range(total_epoch):\n",
        "  train_correct=0\n",
        "  train_total=0\n",
        "  train_loss=0 #loss per epoch\n",
        "\n",
        "  valid_correct=0\n",
        "  valid_total=0\n",
        "  valid_loss=0 #loss per epoch\n",
        "  \n",
        "  net.train() #put the model in training mode\n",
        "  for data in train_loader:\n",
        "\n",
        "    #every data consits of (batch_size)\n",
        "    X,y=data[0].to(device), data[1].to(device) #(X batch_size), label(X batch_size) -> #batch size comes first #note that the label here is a number which is index in labels list\n",
        "    \n",
        "    net.zero_grad()  \n",
        "    output = net(X)  \n",
        "    loss = criterion(output, y) #calculate the error/ loss for the that batch (data)\n",
        "\n",
        "    loss.backward()  #computes dloss/dw for every parameter w  (loss for every parameter)\n",
        "    optimizer.step() #update weights\n",
        "    train_loss+=loss.item()\n",
        "\n",
        "    #calculate how many right do you have in every training data until the end of all training datas\n",
        "    #output is Batch_size*10 tensor\n",
        "    for k, i in enumerate(output): # the output is batch_size* 10 tensor   # k is the index of the data # i the data itself\n",
        "        if torch.argmax(i) == y[k]: # in every row find the highest prediction index and compare it to y[k]\n",
        "                train_correct += 1\n",
        "        train_total += 1\n",
        "\n",
        "  exp_lr_scheduler.step() #learning rate adjustment\n",
        "  \n",
        "  net.eval() #put the model in evaluation mode\n",
        "  #validate for each epoch\n",
        "  with torch.no_grad(): # no gradient\n",
        "    for data in valid_loader:\n",
        "      X, y = data[0].to(device), data[1].to(device) # store the images in X and labels in y\n",
        "      output = net(X) \n",
        "      loss = criterion(output, y)\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "      for k, i in enumerate(output): # the output is batch_size* 10 ARRAY\n",
        "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
        "              valid_correct += 1\n",
        "          valid_total += 1\n",
        "  \n",
        "  #if the model is better than the previous best store it\n",
        "  if((valid_correct/valid_total)>best_valid_acc):\n",
        "    best_valid_acc= (valid_correct/valid_total)\n",
        "    torch.save(net.state_dict(), \"./save_best.pth\") #save early stopping point\n",
        "\n",
        "  if((cur_epoch+1)%(total_epoch*0.1)==0):\n",
        "    print(' Epoch {}/{}: Training Accuracy {} |  Training Loss {} || Validation Accuracy {} |  Validation Loss {}'.format(cur_epoch+1, total_epoch, train_correct/train_total,train_loss/len(train_loader),valid_correct/valid_total,valid_loss/len(valid_loader))) #accuray for each epoch\n",
        "    print(' Best validation so far {}'.format(best_valid_acc))\n",
        "    print('-------------------------------------------------------------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Va5c_L0RB3",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IwVOgoa0Qyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the best validation accuracy model so far\n",
        "load_model = Model().to(device)\n",
        "load_model.load_state_dict(torch.load(\"./save_best.pth\")) \n",
        "\n",
        "load_model.eval()\n",
        "\n",
        "correct =0\n",
        "total=0\n",
        "with torch.no_grad(): # no gradient\n",
        "  for data in test_loader:\n",
        "      X, y = data[0].to(device), data[1].to(device) # store the Xs and labels\n",
        "      output = load_model(X) \n",
        "      for k, i in enumerate(output): # \n",
        "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
        "              correct += 1\n",
        "          total += 1\n",
        "\n",
        "print(\"Test Accuracy: \", correct/total)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
